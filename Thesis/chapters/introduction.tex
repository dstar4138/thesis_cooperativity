\chapter{Introduction}
\index{Introduction@\emph{Introduction}}%
\label{chap:introduction}

Runtime systems can be broken up into multiple distinct parts: the garbage 
collector, dynamic type-checker, resource allocator, and much more. One 
sub-system of a language's run-time is the task-scheduler. The scheduler is 
responsible for order of task evaluation and the distribution of 
these tasks across the available processing units.

Tasks are typically spawned when there is a chance for parallelism, either 
explicitly through \texttt{spawn} or \texttt{fork} commands or implicitly 
through calls to parallel built-in functions like \texttt{pmap}. In either 
case it is assumed that the job of a task is to perform some action concurrent 
to the parent task because it would be quicker if given the chance to be 
parallel.

It is up to the scheduler of these tasks to try and optimize for where there
is opportunity for parallelism. However, it's not as simple as evenly 
distributing the tasks over the set of processing units. Sometimes, these 
tasks need particular resources which other tasks are currently using,
or perhaps some tasks are waiting for user input and don't have anything to
do. Still worse, some tasks may be trying to work together to complete an 
objective, and rely on dynamic dependencies that change over time.

Tasks however, in functional language verbiage, are typically called 
{\em processes} due to the inherent isolation this term brings and the language 
paradigm calls for. Instead, message passing is a common alternative to, and 
sometimes abstraction of, shared
memory. Message passing is akin to emailing a colleague a question. You operate
asynchronously, and your colleague can check her mailbox and then
respond at her leisure. Meanwhile you are free to operate on an assumption 
until proven wrong, wait until she gets back to you, or even ask someone else.

While message passing is a good method for inter-process communication, it is
also a nice mechanism for catching when two processes are working together.
For example, consider a purely functional \texttt{pmap}, where all 
workers are given subsections of the list. Each worker thread will have no
need to access another's subsection and thus no messages will need to be passed.
However, what in the event the function being mapped on a particular subsection uses 
several processes? Each may access a shared resource via message passing. 
We would see a close coupling in this case.
This highlights the granularity of process coupling, in that the \texttt{pmap}
workers exhibit course-grained coupling, which allows the scheduler greater 
flexibility to run them in parallel. The opposite is true for the processes 
which show close coupling, like the mapped function.

There exists a large number of mechanisms that scheduling systems can use in an 
attempt to improve work-load across all processing units. Some of these 
mechanisms use what's called a feedback system. Namely, they observe the 
running behaviour of the application as a whole, (i.e. collect {\em metrics}),
and modify themselves to improve operation. We define the granularity of process
coupling as \textbf{Process Cooperativity}.

Process Cooperativity is an interesting metric by which to grade a system. In
biochemistry the term cooperativity is defined as an increase or decrease
in the rate of interaction between a reactant and a protein as the reactant
concentration increases. We can translate this into an information theoretic 
definition: 
\begin{newdef}\label{def:degree of cooperativity}
    The {\bf degree of cooperativity} of a system is the increase or decrease 
    in the rate of interaction between processes and an inter-process 
    communication method as the concentration of processes fluctuate.
\end{newdef}

Thus, when a process attempts to pass a message to another we know it's trying 
to cooperate on some level. When this frequency of interaction is high, it may
indicate a tight coupling of processes or fine-grained parallelism. If it is 
low, this could indicate course-grained parallelism. In either event, a 
scheduler able to recognize these clusters of cooperative and non-cooperative 
processes should have an edge over those that don't.

Chapter~\ref{chap:background} will look first at the background of classical 
scheduling systems as well as the recent feedback-enabled approaches. Then, we 
will also examine the types of message passing implementations and how these
effect scheduling decisions, now that we are looking at process cooperativity.
Chapter~\ref{chap:methodology} introduces our work on a language and compiler,
built to easily simulate system cooperativity and visualize the effects of 
scheduling mechanisms on these systems. We also discuss a few example mechanics
which take advantage of cooperativity. Some example applications which 
demonstrate different degrees of cooperativity and phase changes are also 
explained.
In Chapter~\ref{chap:results} we run our cooperativity-enabled schedulers along
with a few common non-feedback-enabled schedulers on the example applications
and discuss the results. 
Finally, in Chapter~\ref{chap:conclusions} we give some concluding remarks and
avenues of future research we believe would be fruitful.

