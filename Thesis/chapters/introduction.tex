\chapter{Introduction}
\index{Introduction@\emph{Introduction}}%
\label{chap:introduction}

Runtime systems can be broken up into several distinct parts: the garbage 
collector, dynamic type-checker, resource allocator, \etc~One 
subsystem of a language's runtime is the {\em task scheduler}. The scheduler is 
responsible for ordering task evaluation and the distribution of 
these tasks across the available processing units.

Tasks\footnote{We will be using the terms {\em task}, {\em job}, and {\em process} interchangeably throughout this document.}
are typically spawned when there is a chance for parallelism, either 
explicitly through \texttt{spawn} or \texttt{fork} commands or implicitly 
through calls to parallel built-in functions like \texttt{pmap}. In either 
case it is assumed that the role of a task is to perform some action concurrent 
to the parent task because it would be quicker if given the chance to be 
parallel.
It is up to the scheduler of these tasks to try and optimize for where there
is opportunity for parallelism. However, it's not as simple as evenly 
distributing the tasks over the set of processing units. Sometimes, these 
tasks need particular resources which other tasks are currently using,
or perhaps some tasks are waiting for user input and don't have anything to
do. Still worse, some tasks may be trying to work together to complete an 
objective, and rely on dynamic dependencies that change over time.

Message passing is a common alternative to, and sometimes abstraction of, shared
memory. Some implementations of message passing are akin to emailing a colleague a question. You operate
asynchronously, and your colleague can check her mailbox and then
respond at her leisure. Meanwhile you are free to operate on an assumption 
until proven wrong, wait until she gets back to you, or even ask someone else.
Other implementations require you to wait on your colleague's reply before
continuing.

While message passing is a good method for inter-process communication, it is
also a nice mechanism for catching when two processes are working together.
For example, consider a purely functional \texttt{pmap}, where all 
workers are given subsections of the list. Each worker thread will have no
need to access another's subsection and thus no messages will need to be passed.
However, what happens when the function being mapped on a particular subsection uses 
several processes? Each may access a shared resource via message passing. 
We would observe a close coupling of processes in this case.
This highlights the granularity of process coupling, in that the \texttt{pmap}
workers exhibit coarse-grained parallelism, which allows the scheduler greater 
flexibility to run them in parallel. The opposite is true for the processes 
which show close coupling, like the mapped function. We define this granularity of process
coupling as {\em Process Cooperativity}.

There are many mechanisms that scheduling systems can use to improve workload 
across all processing units. Some of these 
mechanisms use what is called a {\em feedback system}. Namely, they observe the 
running behavior of the application as a whole, (\ie~collect {\em metrics}),
and modify themselves to improve operation. 

{\em Process Cooperativity} is an interesting metric by which to grade a system. In
biochemistry the term cooperativity is defined as an increase or decrease
in the rate of interaction between a reactant and a protein as the reactant
concentration increases. We can translate this into an information theoretic 
definition: 
\begin{newdef}\label{def:degree of cooperativity}
    The degree of cooperativity of a \textbf{process} is the increase or decrease 
    in its rate of interaction with an inter-process communication method, and
    subsequently, the degree of cooperativity of the \textbf{system} is the rate of
    interaction between the sets of processes and channels as the concentration
    of both fluctuate.
\end{newdef}

Thus, when a process attempts to pass a message to another we know it's trying 
to cooperate on some level. When this frequency of interaction is high, it may
indicate a tight coupling of processes or fine-grained parallelism. If it is 
low, this could indicate coarse-grained parallelism. In either event, a 
scheduler able to recognize these clusters of cooperative and non-cooperative 
processes should have an edge over those that don't.

Chapter~\ref{chap:background} will look first at the background of classical 
scheduling systems as well as the recent feedback-enabled approaches. Then, we 
will also examine the types of message passing implementations and how these
effect scheduling decisions, now that we are looking at process cooperativity.
Chapter~\ref{chap:methodology} introduces our work on a language and compiler,
built to easily simulate system cooperativity and visualize the effects of 
scheduling mechanisms on these systems. We also discuss a few example mechanics
which take advantage of cooperativity. Some example applications which 
demonstrate different degrees of cooperativity and phase changes are also 
explained.
In Chapter~\ref{chap:results} we run our cooperativity-enabled schedulers along
with a few common non-feedback-enabled schedulers on the example applications
and discuss the results. 
Finally, in Chapter~\ref{chap:conclusions} we give some concluding remarks and
avenues of future research we believe would be fruitful.

