\chapter{Background}
\index{Background\emph{Background}}%
\label{chap:background}

\section{A Note on Control Theory}
\label{sec:a note on control theory}

Since the formalization of feedback driven systems and the advent of 
Cybernetics, multiple fields have attempted to mold these principles to their 
own models; and run-time schedulers are no exception. This is due, in part, from
process scheduling in parallel systems being fundamentally an NP-Complete 
problem \cite{bruno1976computer}. 

Note that the base case of runtime scheduling is called the Multiprocessor 
Scheduling Problem which is used in job-batch scheduling, and states:
\begin{newdef}
    {\bf MULTIPROCESSOR SCHEDULING PROBLEM}\\ 
    Given a set of jobs $\mathscr{J} = (J_1, J_2, \ldots, J_n)$, a directed 
    acyclic graph (lattice) $L = (\mathscr{J}, C)$ (indicating job dependence,
    and thus precedence constraints), an integer $P$ (the number of processors) 
    and an integer $D$ (the deadline), is there a function $S$ (the schedule)
    mapping $\mathscr{J} \rightarrow \{1, 2, \ldots D\}$ such that:
    \begin{enumerate}
        \item For all $t \leq D, |\{J_i : S(J_i) = t \}| \leq P$. (\# of jobs scheduled per time slice is no more than \# of processors)
        \item If $(J_i, J_j) \in C$, then $S(J_i) < S(J_j)$. (\# jobs cannot be scheduled before their dependencies)
    \end{enumerate}
\end{newdef}
\noindent
In runtime scheduling however, the deadline, $D$, is incremented for each timeslice we
pass. As such, it is possible for $|\mathscr{J}|$ and thus $C$ to fluctuate 
causing a need to re-find $S$. The continuous nature of this problem complicates 
the scheduling problem substantially.
Instead, focus has been more fruitful when pursuing the optimization of various 
measurements using some particular objective function 
\cite{garey1978performance} 
to tune for particular edge cases. As such, scheduling based on such feedback 
metrics is not a new practice \cite{dietz1997use}.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.55]{\detokenize{Feedback_loop_with_descriptions.pdf}} %detoken because of '_' char.
\caption{A classical feedback loop representation.}
\label{fig:controller}
\end{figure}

There is a big distinction though, which can be made between the effects of 
control theory in classical cybernetic applications versus that of run-time 
systems. This is primarily in the adaptation of the controller in the generic 
feedback loop (figure~\ref{fig:controller}).

The classic feedback loop starts by making reading the current state of the 
system and applying some operation to it (via the controller). The operation in
some way affects the system which can be observed by measuring particular 
metrics. These measurements can be fed back to the controller along with 
details about the system's output. If the controller wishes to modify the system
further via the same or an opposite operation, it may do so. The canonical 
example is that of an automobile's cruise control. The controller can correct
the speed of the vehicle by applying or releasing the throttle based on readings
of the current speed.

In typical physical feedback loops there are two scenarios which need to be 
avoided: resonance and rapid compensation. Resonance in physical systems is a 
when a spike in the amplitude of a system's oscillation can cause it to fail at 
a particular frequency. It can be seen that most controller 
models will attempt to damp the adjustments to reduce oscillation which could 
cause resonance or sharp spikes in behavior based on its output. This is due to 
the limitations of the physical space in which they are having to work. But 
frequent or extreme damping or can stress physical systems to the point of
failure as well.

However, in run-time scheduling systems we would very much like to do the 
opposite. We would prefer tight oscillations or consistent behavior of our 
runtime so as to achieve minimal overhead from our modifications. We can also 
compensate, to reach our reference signal, as quickly as we need to as there 
are no physical restrictions for our modifications.

As such these feedback systems are closely coupled with the design of the 
scheduling algorithm, rather than being an interchangeable sensor, and controller
modules. As such we make an effort to trace the feedback optimizations during
our evaluation and explanation of the scheduler designs.

Another distinction must be made as far as the level of foresight the scheduling 
systems have, at least, within this paper. There is a spectrum of clairvoyance 
in classical job-scheduling, in that on one end, job-schedulers have full 
foresight over the jobs which will enter the queue and their order 
(\ie~the full $\mathscr{J}$ set will always be known). These schedulers have the 
opportunity to optimize for future events (by constructing a valid lattice $L$ 
based on the current time $t \leq D$), which is a luxury the scheduling systems 
that this paper discusses do not have.

However, as it is a spectrum, there is a single point of knowledge this subrange 
of schedulers can assume. Namely, that the first job will always be the last, 
and all other jobs will spawn from it. Thus there will always be a single 
process in the queue at the beginning. This is true as the runtime will always 
require an initial primary process (\eg~the `main' function), and once that 
function is completed, the system is terminated (despite the cases of unjoined
children). Apart from this, all other insights will need to be gleaned from the 
evaluation of this initial process.
\todo[inline]{ We mention this due to the implications on our scheduling 
    problem (dynamicly evolving lattice which is altered based on evolving
    cooperation networks and the forking/joinging of child processes), but can 
    we tie this back to the above definition and possibly cooperation?
}

\section{Message-Passing}

In concurrent systems, there are a number of methods for inter-process 
communication. Arguably though, one of the more popular abstractions is the 
idea of message passing. This is especially true in functional languages as the
language assumes shared-nothing by default. Just as compilers can optimize 
using the language constraints, so can the run-time using the implementation.

\begin{figure}[htp]
\centering
\Tree [ .{Message Passing}
			[ .Async 
				Direct 
				Indirect 
			] 
			[ .Sync 
				Asymmetric
				Symmetric 
			]
	   ]
\caption{A High-Level Message-Passing Taxonomy}
\label{fig:mptax}
\end{figure}

Message passing in general can be broken down into two types based on the 
language's implementation; asynchronous or synchronous. In asynchronous message 
passing a process can send the message directly to another process like 
in our example of emailing a coworker, these implementations are aptly named 
mailbox message passing. Another implementation sends the message indirectly  
via a rendezvous point, like a socket.

To send a message in either case requires pushing/copying the 
message into a shared or global memory space for another process to access 
(possibly) at a later time. This push/copy can be done in a lock free manner 
with some lower level atomic data structures such as a double-ended queue. But 
in either a locked or lock-free manner, the process performing the send still 
forces a block somewhere along in the operation, to push the message into some
shared storage. As asynchronous operations require the additional capabilities 
to both store and resend a message at a later time.

In terms of scheduling, a language with asynchronous message passing does not 
hint much in regard to whether progress is being made. If a consumer process
requires a value before continuing and therefore is repeatedly trying to receive 
from the channel, the schedule for the system would be better served by coming 
back to that process at a later time rather than repeatedly looping. However,
asynchronous does benefit from process placement so as to take advantage of 
possible gains in cache affinity \cite{debattista2002cache}. For example, the 
effects of the cache on direct message passing (\eg~a process mailbox) can be 
substantial as two processes on different cores share a location to store and
thus check for content. This shared location if accessed from multiple locations
will have to be updated in possibly multiple locations and validated for 
consistency at the cache level. However, if the two processes are local to the 
same cache there will time saved in context switching and cache-line validation. 
In indirect message passing the task is even worse as multiple processes may 
need access to the same space.

In synchronous message passing, a process must meet another at a provided 
rendezvous point but can either be symmetrical or asymmetrical. Note that the 
rendezvous point is not a requirement in the sense that direct synchronous 
messaging isn't possible. Instead we think of a rendezvous point in synchronous 
communication to be time bound rather than location bound (\ie~two processes are 
blocked until communication occurs, the implementation of this passing is 
irrelevant to this classification).

Asymmetrical message passing is synonymous with Milner's Calculus of 
Communicating Systems \cite{milner1982calculus} or standard $\Pi$-Calculus 
\cite{palamidessi1997comparing}, in that you have a sender, and then a receiver 
which will both block on their respective functions around an anonymous channel 
until the pass has been completed. This differs from a symmetrical message 
passing in that the only operation on the channel is a blocking function which 
swaps values with the process at the other end.

It's worth noting that asynchronous message-passing can be simulated using 
synchronous channels and a secondary buffer process. But by simulating it in 
this fashion we, as the scheduler, elevate the problem of cache locality to a
problem of process locality. The same methods suggested to alleviate some of the 
lost efficiency due to cache locality \cite{markatos1991load,markatos1991memory}
are the same techniques which could be simulated for process locality. Namely
process batching and process affinity.

Note also, it is possible to simulate symmetrical message passing on asymmetrical 
message channels, but in terms of scheduling of synchronizing processes, order 
is now a factor that needs to be considered. On top of this, directionality can 
also be a factor which complicates the channel implementation. Namely, the 
internal queuing of senders or receivers may not percolate hints up to the 
scheduler regarding their queue position. 

For the alternative, symmetrical message passing or swap channels, the order is 
directly handled by the scheduling of the system (\ie~the order at which the 
channels evaluate the $swap$ command can be directly governed). And it is for 
this purpose along with simplifying our core language we have chosen to base our
semantics on symmetric synchronous message-passing. 

\section{Classical Runtime Scheduling}

\todo[inline]{ I would like to discuss current methods of distributing processes
 accross processors, as I am assuming later that readers know of terms like:\\
 - work-stealing \& sharing \\
 - round-robin \\
 - global/local process queues\\
 - spawning and joining methods\\
 Then somehow lead into process dependencies and thus process synchronization. 
This will tie it over to the message-passing section.
}

\section{Feedback-Enabled Scheduling}

\todo[inline]{This is the "related work" section, I would like to give the two
    big examples i've been considering: CML, and Occam-$\pi$.
}

\subsection{Cooperativity as a Metric}

\todo[inline]{This should give the primary definitions of a 
    $cooperative process$ and the differences between focusing on cooperativity 
    rather than interactivity, etc. It would also be a good idea to talk about 
    its relationship with application phasses here.
}


